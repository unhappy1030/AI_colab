{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","mount_file_id":"1uU9z-kFI9Qp2Fkl2Zrgb_GTd3wtZTEe-","authorship_tag":"ABX9TyNaZwm2JsM3/pAtsN2CCAAu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"yvDne98G4krb"},"outputs":[],"source":["# imports\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"]},{"cell_type":"code","source":["# transforms\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))])\n","#transforms.Compose : 여러 전처리 과정을 순서대로 묶어주는 역할\n","#transforms.ToTensor() : Image data를 Tensor를 변환 변환 결과 [0, 255] -> [0, 1.0]\n","#transforms.Normalize() : (x - mean) / std로 변환 mean = 0.5, std = 0.5 변환 결과 [0, 1.0] -> [-1.0, 1.0]\n","\n","#코드 변경해서 정규화 두가지 비교해보기 0~1 , -1~1\n","\n","\n","# datasets 데이터 셋 가져오기\n","trainset = torchvision.datasets.FashionMNIST('./data',\n","    download=True,\n","    train=True,\n","    transform=transform)\n","testset = torchvision.datasets.FashionMNIST('./data',\n","    download=True,\n","    train=False,\n","    transform=transform)"],"metadata":{"id":"OUcpHTjH45Yw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataloader를 사용하여 학습데이터와 테스트 데이터 설정\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n","                                        shuffle=True, num_workers=2)\n","\n","\n","testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n","                                        shuffle=False, num_workers=2)\n","\n","# constant for classes 10개의 결과 라벨\n","classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n","        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n","\n","# helper function to show an image\n","# (used in the `plot_classes_preds` function below)\n","def matplotlib_imshow(img, one_channel=False): #one_channel : 흑백이미지로 처리 할지, 기본적으로 컬러 이미지를 default로 설정\n","    print(img)\n","    print(img.shape)\n","    if one_channel:\n","        img = img.mean(dim=0) # img구성 [C, H, W](채널,높이,너비)형태, dim=0 RGB채널의 평균값을 계산하여 단일채널로 압축\n","    img = img / 2 + 0.5     # unnormalize [-1,1] -> [0,1]\n","    npimg = img.numpy()\n","    if one_channel:\n","        plt.imshow(npimg, cmap=\"Greys\")\n","    else:\n","        plt.imshow(np.transpose(npimg, (1, 2, 0)))"],"metadata":{"id":"AmDoll9L5GHW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 16 * 4 * 4)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","\n","net = Net()"],"metadata":{"id":"swDs98r45MQC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"],"metadata":{"id":"EUS3A8wt5ODe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.tensorboard import SummaryWriter\n","\n","# default `log_dir` is \"runs\" - we'll be more specific here\n","writer = SummaryWriter('runs/fashion_mnist_experiment_1')"],"metadata":{"id":"yHftwW-U5Rk-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get some random training images\n","dataiter = iter(trainloader)\n","images, labels = next(dataiter)\n","\n","# create grid of images\n","img_grid = torchvision.utils.make_grid(images)\n","\n","# show images\n","matplotlib_imshow(img_grid, one_channel=True)\n","\n","# write to tensorboard\n","writer.add_image('four_fashion_mnist_images', img_grid)"],"metadata":{"id":"BTeQd6dT5T8G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["writer.add_graph(net, images)\n","writer.close()"],"metadata":{"id":"gA6dpEvz6J3L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# helper function\n","def select_n_random(data, labels, n=100):\n","    '''\n","    Selects n random datapoints and their corresponding labels from a dataset\n","    '''\n","    assert len(data) == len(labels)\n","\n","    perm = torch.randperm(len(data))\n","    return data[perm][:n], labels[perm][:n]\n","\n","# select random images and their target indices\n","images, labels = select_n_random(trainset.data, trainset.targets)\n","\n","# get the class labels for each image\n","class_labels = [classes[lab] for lab in labels]\n","\n","# log embeddings\n","features = images.view(-1, 28 * 28)\n","writer.add_embedding(features,\n","                    metadata=class_labels,\n","                    label_img=images.unsqueeze(1))\n","writer.close()"],"metadata":{"id":"HzDEpuDc6hJW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!kill 681"],"metadata":{"id":"eW3GrZsi8bDa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%load_ext tensorboard\n","%tensorboard --logdir=runs\n"],"metadata":{"id":"ntBVpykL5gUK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd /content/drive/MyDrive/colab_repo/AI_colab"],"metadata":{"id":"YIZ6lzZJHO3x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git config --global user.email \"\""],"metadata":{"id":"XHgqjKz2Hw2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git add .\n","!git commit -m \"learn Conv2d part in model define\"\n","!git push origin main"],"metadata":{"id":"WT_Pbd7NHaYy"},"execution_count":null,"outputs":[]}]}